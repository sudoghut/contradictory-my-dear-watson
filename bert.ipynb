{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sudos\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 2.95MB/s]\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 23.0kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 375kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                            premise  \\\n",
      "0  5130fd2cb5  and these comments were considered in formulat...   \n",
      "1  5b72532a0b  These are issues that we wrestle with in pract...   \n",
      "2  3931fbe82a  Des petites choses comme celles-là font une di...   \n",
      "3  5622f0c60b  you know they can't really defend themselves l...   \n",
      "4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
      "\n",
      "                                          hypothesis lang_abv language  label  \n",
      "0  The rules developed in the interim were put to...       en  English      0  \n",
      "1  Practice groups are not permitted to work on t...       en  English      2  \n",
      "2              J'essayais d'accomplir quelque chose.       fr   French      0  \n",
      "3  They can't defend themselves because of their ...       en  English      0  \n",
      "4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  \n",
      "0    5\n",
      "2    2\n",
      "1    2\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('https://raw.githubusercontent.com/sudoghut/contradictory-my-dear-watson/main/data/train_10.csv')\n",
    "print(df_raw.head(5))\n",
    "print(df_raw[\"label\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'Entailment':0,\n",
    "          'Neutral':1,\n",
    "          'Contradiction':2,\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = df['label']\n",
    "        self.premise = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['premise']]\n",
    "        self.hypothesis = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['hypothesis']]\n",
    "        self.texts = self.premise + self.hypothesis\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1 1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df_raw.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df_raw)), int(.9*len(df_raw))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 416M/416M [00:15<00:00, 28.3MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sudos\\Dropbox\\single_projects\\kaggle\\contradictory-my-dear-watson\\bert.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m model \u001b[39m=\u001b[39m BertClassifier()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m LR \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m train(model, df_train, df_val, LR, EPOCHS)\n",
      "\u001b[1;32mc:\\Users\\sudos\\Dropbox\\single_projects\\kaggle\\contradictory-my-dear-watson\\bert.ipynb Cell 6\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, train_data, val_data, learning_rate, epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train, val \u001b[39m=\u001b[39m Dataset(train_data), Dataset(val_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     val_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(val, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\sudos\\Dropbox\\single_projects\\kaggle\\contradictory-my-dear-watson\\bert.ipynb Cell 6\u001b[0m in \u001b[0;36mDataset.__init__\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, df):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m [labels[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpremise \u001b[39m=\u001b[39m [tokenizer(text, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                            padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                             return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mpremise\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypothesis \u001b[39m=\u001b[39m [tokenizer(text, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                            padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                             return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mhypothesis\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[1;32mc:\\Users\\sudos\\Dropbox\\single_projects\\kaggle\\contradictory-my-dear-watson\\bert.ipynb Cell 6\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, df):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m [labels[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpremise \u001b[39m=\u001b[39m [tokenizer(text, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                            padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                             return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mpremise\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypothesis \u001b[39m=\u001b[39m [tokenizer(text, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                            padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sudos/Dropbox/single_projects/kaggle/contradictory-my-dear-watson/bert.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                             return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mhypothesis\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "047a54d6d5df71c45a9c123b62e463480937259cb8383da567628524d0beb982"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
